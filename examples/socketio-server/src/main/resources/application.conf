akka {
  loglevel = INFO
  # event-handlers = ["akka.event.slf4j.Slf4jEventHandler"]
  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    auto-down-unreachable-after = 10s
  }

  persistence {
    journal.plugin = "akka.persistence.journal.leveldb-shared"
    journal.leveldb-shared.store {
      # DO NOT USE 'native = off' IN PRODUCTION !!!
      native = off
      dir = "target/shared-journal"
    }
    snapshot-store.local.dir = "target/snapshots"
  }
}

# Settings for the ClusterShardingExtension
akka.contrib.cluster.sharding {
  # The extension creates a top level actor with this name in top level user scope,
  # e.g. '/user/sharding'
  guardian-name = sharding
  # Start the coordinator singleton manager on members tagged with this role.
  # All members are used if undefined or empty.
  # ShardRegion actor is started in proxy only mode on nodes that are not tagged
  # with this role.
  role = ""
  # The ShardRegion retries registration and shard location requests to the
  # ShardCoordinator with this interval if it does not reply.
  retry-interval = 2 s
  # Maximum number of messages that are buffered by a ShardRegion actor.
  buffer-size = 100000
  # Timeout of the shard rebalancing process.
  handoff-timeout = 60 s
  # Rebalance check is performed periodically with this interval.
  rebalance-interval = 10 s
  # How often the coordinator saves persistent snapshots, which are
  # used to reduce recovery times
  snapshot-interval = 10 s
  # Setting for the default shard allocation strategy
  least-shard-allocation-strategy {
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    rebalance-threshold = 10
    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }
}

# Settings for the DistributedPubSubExtension
akka.contrib.cluster.pub-sub {
  # Actor name of the mediator actor, /user/distributedPubSubMediator
  name = distributedPubSubMediator

  # Start the mediator on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""

  # The routing logic to use for 'Send'
  # Possible values: random, round-robin, consistent-hashing, broadcast
  routing-logic = random

  # How often the DistributedPubSubMediator should send out gossip information
  gossip-interval = 1s

  # Removed entries are pruned after this duration
  removed-time-to-live = 120s

  # Maximum number of elements to transfer in one message when synchronizing the registries.
  # Next chunk will be transferred in next round of gossip.
  max-delta-elements = 3000

}

# check the reference.conf in spray-can/src/main/resources for all defined settings
spray.can.server {
  # uncomment the next line for making this an HTTPS example
  #ssl-encryption = on
  idle-timeout = 30 s
  request-timeout = 10 s

  request-chunk-aggregation-limit = 0

  parsing.max-content-length = 5g
  parsing.incoming-auto-chunking-threshold-size = 45k
}

spray.can.client {
  connecting-timeout = 100s
}

spray.socketio {

  mode = "local"

  server {
    supported-transports = "websocket,xhr-polling"
    #supported-transports = "xhr-polling"

    heartbeat-interval = 30
    heartbeat-timeout = 60
    close-timeout = 60

    # seconds
    actor-selection-resolve-timeout = 10

    # seconds
    namespace-subscribe-timeout = 30

    namespace-group-name = "ns-group"
  }

  seed-nodes = []

  # Fully qualified config path which holds the dispatcher configuration
  # to be used for the Namespaces actor.
  namespaces-dispatcher = "akka.actor.default-dispatcher"

  # Fully qualified config path which holds the dispatcher configuration
  # to be used for the Namespace actors.
  namespace-dispatcher = "akka.actor.default-dispatcher"
}

spray.socketio.benchmark {
  server {
    host = "localhost"
    port = 8080
  }

  client {
    addresses = ["localhost:8080"]

    concurrencyLevels = [10, 100, 500, 1000, 2000, 5000, 10000, 30000]

    post-test-reception-timeout = 5

    create-bunch-clients-timeout = 30

    initail-messages-sent-per-second = 1

    max-messages-sent-per-second = 200000

    messages-sent-per-second-ramp = 100

    seconds-to-test-each-load-state = 1

    seconds-between-rounds = 2
  }

  broadcast = false
}

